{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "from scipy import misc, stats\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.utils import to_categorical # For keras > 2.0\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (100,100)\n",
    "N_SAMPLE = 600 # how many to sample from each class (classes w/ <N will be dropped)\n",
    "N_CLASSES = 5 # how many asanas to classify\n",
    "TRAIN_TEST_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dataset/google/Setu+Bandha+Sarvangasana/Scrapper_297.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_241.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_295.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_83.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_337.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_111.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_312.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_345.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_19.jpg',\n",
       " 'dataset/google/Setu+Bandha+Sarvangasana/Scrapper_167.jpg']"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all image files\n",
    "filepaths = glob.glob('dataset/google/*/*')\n",
    "filepaths = [x for x in filepaths if os.path.splitext(x)[1]!='.json']\n",
    "print(len(filepaths))\n",
    "filepaths[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71379, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Setu+Bandha+Sarvangasana</td>\n",
       "      <td>dataset/google/Setu+Bandha+Sarvangasana/Scrapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Setu+Bandha+Sarvangasana</td>\n",
       "      <td>dataset/google/Setu+Bandha+Sarvangasana/Scrapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Setu+Bandha+Sarvangasana</td>\n",
       "      <td>dataset/google/Setu+Bandha+Sarvangasana/Scrapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setu+Bandha+Sarvangasana</td>\n",
       "      <td>dataset/google/Setu+Bandha+Sarvangasana/Scrapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Setu+Bandha+Sarvangasana</td>\n",
       "      <td>dataset/google/Setu+Bandha+Sarvangasana/Scrapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                               path\n",
       "0  Setu+Bandha+Sarvangasana  dataset/google/Setu+Bandha+Sarvangasana/Scrapp...\n",
       "1  Setu+Bandha+Sarvangasana  dataset/google/Setu+Bandha+Sarvangasana/Scrapp...\n",
       "2  Setu+Bandha+Sarvangasana  dataset/google/Setu+Bandha+Sarvangasana/Scrapp...\n",
       "3  Setu+Bandha+Sarvangasana  dataset/google/Setu+Bandha+Sarvangasana/Scrapp...\n",
       "4  Setu+Bandha+Sarvangasana  dataset/google/Setu+Bandha+Sarvangasana/Scrapp..."
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe of filenames\n",
    "filepaths_df = pd.DataFrame({'path': filepaths,\\\n",
    "                            'label': [x.split('/')[2] for x in filepaths]})\n",
    "\n",
    "print(filepaths_df.shape)\n",
    "filepaths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those classes with <N_SAMPLE labels\n",
    "# pick N_CLASSES of those remaining \n",
    "\n",
    "cnt = filepaths_df.groupby('label', as_index=False)['path'].count()\n",
    "lbls = cnt[cnt.path>=N_SAMPLE].label.tolist() # pick only those w/ cnt>N_SAMPLE\n",
    "lbls = random.sample(lbls,N_CLASSES) # pick randomly N_CLASSES\n",
    "mask = filepaths_df['label'].isin(lbls)\n",
    "filepaths_df = filepaths_df[mask]\n",
    "\n",
    "\n",
    "# print filepaths_df.shape\n",
    "# filepaths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Savasana</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supta+Balasana</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Svara+Yoga+Pranayama</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ustrasana</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  label  path\n",
       "0                Dhyana   673\n",
       "1              Savasana   688\n",
       "2        Supta+Balasana   614\n",
       "3  Svara+Yoga+Pranayama   704\n",
       "4             Ustrasana   606"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt#[cnt.path==cnt.path.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>dataset/google/Dhyana/Scrapper_626.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>dataset/google/Dhyana/Scrapper_256.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>dataset/google/Dhyana/Scrapper_624.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>dataset/google/Dhyana/Scrapper_653.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dhyana</td>\n",
       "      <td>dataset/google/Dhyana/Scrapper_5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                    path\n",
       "0  Dhyana  dataset/google/Dhyana/Scrapper_626.jpg\n",
       "1  Dhyana  dataset/google/Dhyana/Scrapper_256.jpg\n",
       "2  Dhyana  dataset/google/Dhyana/Scrapper_624.jpg\n",
       "3  Dhyana  dataset/google/Dhyana/Scrapper_653.jpg\n",
       "4  Dhyana    dataset/google/Dhyana/Scrapper_5.jpg"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample N_SAMPLE per group\n",
    "filepaths_sample_df = filepaths_df.groupby('label').apply(lambda x: \\\n",
    "                                        x.sample(N_SAMPLE)).reset_index(drop=True)\n",
    "print(filepaths_sample_df.shape)\n",
    "filepaths_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read in: dataset/google/Dhyana/Scrapper_402.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_619.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_110.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_515.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_280.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_124.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_603.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_50.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_632.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_580.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_251.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Dhyana/Scrapper_170.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Supta+Balasana/Scrapper_372.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_282.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_669.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_434.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_413.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_198.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_436.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_402.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_117.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_373.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_425.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Svara+Yoga+Pranayama/Scrapper_420.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Ustrasana/Scrapper_414.jpg, Dropping from dataframe\n",
      "Failed to read in: dataset/google/Ustrasana/Scrapper_594.jpg, Dropping from dataframe\n",
      "\n",
      "Number of images loaded: 2974\n",
      "Number of images in df: 2974\n"
     ]
    }
   ],
   "source": [
    "# Load the images, drop failed loads from df\n",
    "\n",
    "images=[]\n",
    "for i in range(len(filepaths_sample_df.path)):\n",
    "    path = filepaths_sample_df.path[i]\n",
    "    try:\n",
    "        images.append(misc.imread(path))\n",
    "    except:\n",
    "        print \"Failed to read in: %s, Dropping from dataframe\" % path\n",
    "        filepaths_sample_df = filepaths_sample_df.drop(i)\n",
    "print\n",
    "print(\"Number of images loaded: %d\" %len(images))\n",
    "print(\"Number of images in df: %d\" %filepaths_sample_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing sample results for scaling/reszing of image 2217, dataset/google/Svara+Yoga+Pranayama/Scrapper_445.jpg\n",
      "\n",
      "..... BEFORE:\n",
      "(283, 180, 3)\n",
      "(283, 180)\n",
      "DescribeResult(nobs=50940, minmax=(0, 255), mean=229.476423243031, variance=1501.5965897311469, skewness=-3.0836850610053985, kurtosis=10.807348496126243)\n",
      "\n",
      "Processed 0 images...\n",
      "Processed 250 images...\n",
      "Processed 500 images...\n",
      "Processed 750 images...\n",
      "Processed 1000 images...\n",
      "Processed 1250 images...\n",
      "Processed 1500 images...\n",
      "Processed 1750 images...\n",
      "Processed 2000 images...\n",
      "Processed 2250 images...\n",
      "Processed 2500 images...\n",
      "Processed 2750 images...\n",
      "\n",
      "..... AFTER:\n",
      "(100, 100, 3)\n",
      "(100, 100)\n",
      "DescribeResult(nobs=10000, minmax=(0.003921569, 1.0), mean=0.8987503, variance=0.022363525, skewness=-3.04746413230896, kurtosis=10.699436528348434)\n"
     ]
    }
   ],
   "source": [
    "# (1) Scale image arrays s.t range is between 0 and 1 instea dof 0 and 255\n",
    "# (2) Resize to be of dim IMG_SIZE (width,height)\n",
    "# When the normType is NORM_MINMAX, cv::normalize normalizes _src in such a way that \n",
    "#   the min value of dst is alpha and max value of dst is beta. cv::normalize does its magic \n",
    "#   using only scales and shifts (i.e. adding constants and multiplying by constants).\n",
    "# (3) Drop fourth dimmension for PNG images\n",
    "# (4) create 3rd dim for greay scale imges\n",
    "i=random.randint(0, len(images))\n",
    "print(\"Showing sample results for scaling/reszing of image %d, %s\" % \\\n",
    "      (i,filepaths_sample_df.path[i]))\n",
    "print\n",
    "print \"..... BEFORE:\"\n",
    "print images[i].shape\n",
    "print images[i][:,:,0].shape\n",
    "print stats.describe(images[i][:,:,0].flatten())\n",
    "print\n",
    "\n",
    "images_sc = [None] * len(images)\n",
    "for j in range(len(images)):\n",
    "    if j % 250 == 0:\n",
    "        print \"Processed %d images...\" % j\n",
    "    if images[j].all()==None:\n",
    "        images_sc[j]=None\n",
    "    else:\n",
    "        try:\n",
    "            temp = images[j]\n",
    "            if len(temp.shape) > 2 and temp.shape[2] == 4: # PNG rgb images have 4 channels\n",
    "                temp = cv2.cvtColor(temp, cv2.COLOR_BGRA2BGR)\n",
    "            elif len(temp.shape) > 2 and temp.shape[2] == 2: # PNG grsc images have 2 channels\n",
    "                temp = np.stack((temp[:,:,0],)*3, -1)\n",
    "            elif len(temp.shape) == 2: # grsc images have 1 channel\n",
    "                temp = np.stack((temp,)*3, -1)\n",
    "            temp = cv2.resize(temp.astype('uint8'), dsize=IMG_SIZE)\n",
    "            temp = cv2.normalize(temp, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, \\\n",
    "                                 dtype=cv2.CV_32F, dst=None)\n",
    "            images_sc[j] = temp\n",
    "        except:\n",
    "            print \"Unexpected error:\", sys.exc_info()[0]\n",
    "\n",
    "print   \n",
    "print \"..... AFTER:\"     \n",
    "print images_sc[i].shape\n",
    "print images_sc[i][:,:,0].shape\n",
    "print stats.describe(images_sc[i][:,:,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2974 images loaded and 2974 labels\n",
      "\n",
      "... of which 2379 are train images loaded and 2379 train labels\n",
      "\n",
      "... of which 595 train images loaded and 595 train labels\n"
     ]
    }
   ],
   "source": [
    "# Split at the given index\n",
    "n_images = len(images_sc)\n",
    "labels = filepaths_sample_df.label.tolist()\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_Y = encoder.transform(labels)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = to_categorical(encoded_Y)\n",
    "\n",
    "paths = filepaths_sample_df.path.tolist()\n",
    "print(\"There are %d images loaded and %d labels\" % \\\n",
    "      (n_images,len(labels)))\n",
    "print\n",
    "\n",
    "split_index = int(TRAIN_TEST_SPLIT * n_images)\n",
    "shuffled_indices = np.random.permutation(n_images)\n",
    "train_indices = shuffled_indices[0:split_index]\n",
    "test_indices = shuffled_indices[split_index:]\n",
    "\n",
    "# Split the images and the labels\n",
    "x_train = np.array([images_sc[i] for i in train_indices])\n",
    "y_train = np.array([dummy_y[i] for i in train_indices])\n",
    "paths_train = [paths[i] for i in train_indices]\n",
    "print(\"... of which %d are train images loaded and %d train labels\" % \\\n",
    "      (x_train.shape[0],y_train.shape[0]))\n",
    "print\n",
    "\n",
    "x_test = np.array([images_sc[i] for i in test_indices])\n",
    "y_test = np.array([dummy_y[i] for i in test_indices])\n",
    "paths_test = [paths[i] for i in test_indices]\n",
    "print(\"... of which %d train images loaded and %d train labels\" % \\\n",
    "      (x_test.shape[0],y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/google/Savasana/Scrapper_156.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa243628ad0>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=random.randint(0, x_train.shape[0])\n",
    "print paths_train[i]\n",
    "plt.imshow(x_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(positive_images, negative_images):\n",
    "    # INPUTS\n",
    "    # positive_images - Images where the label = 1 (True)\n",
    "    # negative_images - Images where the label = 0 (False)\n",
    "\n",
    "    figure = plt.figure()\n",
    "    count = 0\n",
    "    for i in range(positive_images.shape[0]):\n",
    "        count += 1\n",
    "        figure.add_subplot(2, positive_images.shape[0], count)\n",
    "        plt.imshow(positive_images[i, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"1\")\n",
    "\n",
    "        figure.add_subplot(1, negative_images.shape[0], count)\n",
    "        plt.imshow(negative_images[i, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of positive and negative examples to show\n",
    "# N_TO_VISUALIZE = 10\n",
    "\n",
    "# # Select the first N positive examples\n",
    "# positive_example_indices = (y_train == 1)\n",
    "# positive_examples = x_train[positive_example_indices, :, :]\n",
    "# positive_examples = positive_examples[0:N_TO_VISUALIZE, :, :]\n",
    "\n",
    "# # Select the first N negative examples\n",
    "# negative_example_indices = (y_train == 0)\n",
    "# negative_examples = x_train[negative_example_indices, :, :]\n",
    "# negative_examples = negative_examples[0:N_TO_VISUALIZE, :, :]\n",
    "\n",
    "# # Call the visualization function\n",
    "# visualize_data(positive_examples, negative_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MODEL CREATION ##\n",
    "# # Imports\n",
    "# # Model Hyperparamaters\n",
    "# N_LAYERS = 4\n",
    "\n",
    "# def cnn(size, n_layers, n_classes):\n",
    "#     # INPUTS\n",
    "#     # size     - size of the input images\n",
    "#     # n_layers - number of layers\n",
    "#     # OUTPUTS\n",
    "#     # model    - compiled CNN\n",
    "\n",
    "#     # Define model hyperparamters\n",
    "#     MIN_NEURONS = 20\n",
    "#     MAX_NEURONS = 120\n",
    "#     KERNEL = (3, 3)\n",
    "\n",
    "#     # Determine the # of neurons in each convolutional layer\n",
    "#     steps = np.floor(MAX_NEURONS / (n_layers + 1))\n",
    "#     nuerons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n",
    "#     nuerons = nuerons.astype(np.int32)\n",
    "\n",
    "#     # Define a model\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Add convolutional layers\n",
    "#     for i in range(0, n_layers):\n",
    "#         if i == 0:\n",
    "#             shape = (size[0], size[1], size[2])\n",
    "#             model.add(Conv2D(nuerons[i], KERNEL, input_shape=shape))\n",
    "#         else:\n",
    "#             model.add(Conv2D(nuerons[i], KERNEL))\n",
    "\n",
    "#         model.add(Activation('relu'))\n",
    "\n",
    "#     # Add max pooling layer with dropout\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "# #     model.add(Dense(MAX_NEURONS))\n",
    "# #     model.add(Activation('relu'))\n",
    "\n",
    "#     # Add output layer\n",
    "# #     model.add(Dense(1))\n",
    "# #     model.add(Activation('sigmoid'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation = \"relu\"))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(n_classes, activation = \"softmax\"))\n",
    "\n",
    "#     # Compile the model\n",
    "# #     model.compile(loss='binary_crossentropy',\n",
    "# #                   optimizer='adam',\n",
    "# #                   metrics=['accuracy'])\n",
    "#     optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#     model.compile(optimizer = optimizer , \\\n",
    "#               loss = \"categorical_crossentropy\", \\\n",
    "#               metrics=[\"accuracy\"])\n",
    "\n",
    "#     # Print a summary of the model\n",
    "#     model.summary()\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "image_size = x_train[0].shape\n",
    "n_classes = y_train.shape[1]\n",
    "# model = cnn(size=image_size, n_layers=N_LAYERS, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 86\n",
    "\n",
    "img_rows, img_cols = IMG_SIZE\n",
    "\n",
    "nb_filters_1 = 32 \n",
    "nb_filters_2 = 64 \n",
    "nb_filters_3 = 128 \n",
    "\n",
    "nb_conv = 3 # kernel_size dim\n",
    "nb_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_43 (Conv2D)           (None, 100, 100, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 100, 100, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 50, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 10,307,109\n",
      "Trainable params: 10,307,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(filters = nb_filters_1, \n",
    "                 kernel_size = (nb_conv,nb_conv),\n",
    "                 padding = 'Same', \n",
    "                 activation ='relu', \n",
    "                 input_shape = image_size))\n",
    "model1.add(Conv2D(filters = nb_filters_1, \n",
    "                 kernel_size = (nb_conv,nb_conv),\n",
    "                 padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Conv2D(filters = nb_filters_2, \n",
    "                 kernel_size = (nb_conv,nb_conv),\n",
    "                 padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model1.add(Conv2D(filters = nb_filters_2, \n",
    "                 kernel_size = (nb_conv,nb_conv),\n",
    "                 padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256, activation = \"relu\"))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(nb_classes, activation = \"softmax\"))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model1.compile(optimizer = optimizer , \\\n",
    "              loss = \"categorical_crossentropy\", \\\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL TRAINING ##\n",
    "# Training Hyperparamters\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# Early stopping callback\n",
    "PATIENCE = 10\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=PATIENCE, verbose=0, mode='auto')\n",
    "\n",
    "# TensorBoard callback\n",
    "LOG_DIRECTORY_ROOT = 'logdir'\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"{}/run-{}/\".format(LOG_DIRECTORY_ROOT, now)\n",
    "tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n",
    "\n",
    "# Place the callbacks in a list\n",
    "callbacks = [early_stopping, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 59s - loss: 2.5611 - acc: 0.2114\n",
      "Epoch 2/5\n",
      " - 58s - loss: 2.3321 - acc: 0.2215\n",
      "Epoch 3/5\n",
      " - 58s - loss: 1.9752 - acc: 0.2215\n",
      "Epoch 4/5\n",
      " - 59s - loss: 1.6849 - acc: 0.2186\n",
      "Epoch 5/5\n",
      " - 58s - loss: 1.6120 - acc: 0.2182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2450f1e10>"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model1.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\\\n",
    "          callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "MODEL_DIRECTORY_ROOT = 'modeldir'\n",
    "model_dir = \"{}/run-{}/\".format(MODEL_DIRECTORY_ROOT, now)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    " \n",
    "save_model(model1, model_dir+'model.h5', overwrite=True,include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL EVALUATION ##\n",
    "# Make a prediction on the test set\n",
    "test_predictions = model1.predict(x_test)\n",
    "test_predictions = np.round(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595, 100, 100, 3)\n",
      "(595, 5)\n",
      "DescribeResult(nobs=595, minmax=(array([0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0.], dtype=float32)), mean=array([0., 0., 0., 0., 0.], dtype=float32), variance=array([0., 0., 0., 0., 0.], dtype=float32), skewness=array([0., 0., 0., 0., 0.], dtype=float32), kurtosis=array([-3., -3., -3., -3., -3.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print x_test.shape\n",
    "print test_predictions.shape\n",
    "print stats.describe(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Report the accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-532-c995a3c459d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mvisualize_incorrect_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-532-c995a3c459d9>\u001b[0m in \u001b[0;36mvisualize_incorrect_labels\u001b[0;34m(x_data, y_real, y_predicted)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", Real: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_incorrect_labels(x_data, y_real, y_predicted):\n",
    "    # INPUTS\n",
    "    # x_data      - images\n",
    "    # y_data      - ground truth labels\n",
    "    # y_predicted - predicted label\n",
    "    count = 0\n",
    "    figure = plt.figure()\n",
    "    incorrect_label_indices = (y_real != y_predicted)\n",
    "    y_real = y_real[incorrect_label_indices]\n",
    "    y_predicted = y_predicted[incorrect_label_indices]\n",
    "    x_data = x_data[incorrect_label_indices, :, :, :]\n",
    "\n",
    "    maximum_square = np.ceil(np.sqrt(x_data.shape[0]))\n",
    "\n",
    "    for i in range(x_data.shape[0]):\n",
    "        count += 1\n",
    "        figure.add_subplot(maximum_square, maximum_square, count)\n",
    "        plt.imshow(x_data[i, :, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Predicted: \" + str(int(y_predicted[i])) + \", Real: \" + str(int(y_real[i])), fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_incorrect_labels(x_test, y_test, np.asarray(test_predictions).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
